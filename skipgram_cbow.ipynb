{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP exercise Lab2\n",
        "# Name: Trao An Huy\n",
        "# MSSV: 22280041"
      ],
      "metadata": {
        "id": "l7H6aWJa4in7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word embedding and one-hot encoding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sbWNBFv5ineh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-hot encoding\n",
        "\n",
        "> One-hot encoding is the process of turning categorical factors into a numerical structure that machine learning algorithms can readily process. It functions by representing each category in a feature as a binary vector of 1s and 0s, with the vector's size equivalent to the number of potential categories."
      ],
      "metadata": {
        "id": "PfCcod1xoDoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']"
      ],
      "metadata": {
        "id": "tl5VgYs_ipju"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-hot integer encoding"
      ],
      "metadata": {
        "id": "ZVqliCz4njHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(np.array(data))\n",
        "\n",
        "print(data)\n",
        "print(integer_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf3v1gLInkwK",
        "outputId": "c5840a74-db0e-4150-9327-df875f26c1b1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
            "[0 0 2 0 1 1 2 0 2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-hot binary encoding"
      ],
      "metadata": {
        "id": "C_eLAP-Mn9Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded_data = one_hot_encoder.fit_transform(integer_encoded)\n",
        "\n",
        "print(data)\n",
        "print(onehot_encoded_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8zKywxqn7-8",
        "outputId": "d503a33c-b4b1-4b55-9327-3108399bd304"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1\n",
        "What are the limitations of one-hot encoding?\n",
        "\n",
        "## ANSWER .\n",
        "- Curse of Dimensionality (High Cardinality Problem):\n",
        "\n",
        "Explanation: When a categorical feature has a very large number of unique categories (high cardinality), one-hot encoding creates a new binary feature (column) for each category. This drastically increases the number of features (dimensionality) in the dataset.\n",
        "\n",
        "Impact: High dimensionality can lead to increased computational cost (memory and processing time), make models harder to train, potentially require more data to generalize well, and increase the risk of overfitting. Imagine one-hot encoding a 'ZIP Code' or 'City' column in a large dataset – you could add thousands of new columns.\n",
        "\n",
        "- Multicollinearity:\n",
        "\n",
        "Explanation: The newly created binary features are often highly correlated. Specifically, if you know the values of k-1 columns for a given feature (where k is the number of categories), you automatically know the value of the k-th column (it will be 1 if all others are 0, and 0 if one of the others is 1). This is sometimes called the \"dummy variable trap\".\n",
        "\n",
        "Impact: While many algorithms can handle this, it can be problematic for some models, particularly linear models like Linear Regression or Logistic Regression. It can destabilize coefficient estimates and make their interpretation difficult. Often, one of the one-hot encoded columns is dropped to avoid perfect multicollinearity.\n",
        "\n",
        "- Increased Sparsity:\n",
        "\n",
        "Explanation: After one-hot encoding, each original data point will have a '1' in only one of the newly created columns for that feature, and '0's in all the others. This results in a dataset matrix that is mostly filled with zeros (sparse).\n",
        "\n",
        "Impact: While sparse matrices can be stored efficiently using specialized formats, processing them can still be computationally intensive if not handled correctly. Some algorithms might not perform optimally on highly sparse data.\n",
        "\n",
        "- Loss of Ordinal Information (If Applicable):\n",
        "\n",
        "Explanation: If the original categorical variable has an inherent order (e.g., 'low', 'medium', 'high'; 'cold', 'warm', 'hot'), one-hot encoding treats each category as independent and equally different from all others. It doesn't preserve the ordinal relationship between the categories.\n",
        "\n",
        "Impact: The model loses potentially valuable information about the ranking or order between categories. For such cases, other encoding methods like Ordinal Encoding might be more appropriate (though Ordinal Encoding has its own limitations, like implying equal distance between ranks).\n",
        "\n",
        "- Handling New/Unseen Categories:\n",
        "\n",
        "Explanation: If the model encounters a category in new data (e.g., test set or during prediction) that was not present in the training data used to fit the encoder, the encoder won't have a corresponding column for it.\n",
        "\n",
        "Impact: This can cause errors during the transformation process. Strategies are needed to handle this, such as ignoring the category (resulting in all zeros for that feature's columns), assigning it to a predefined 'other' category (if planned during training), or raising an error."
      ],
      "metadata": {
        "id": "gj-KxAzsyN67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embedding\n",
        "\n",
        "ELI5 for word embeddings\n",
        "> The word embeddings can be thought of as a child’s understanding of the words. Initially, the word embeddings are randomly initialized and they don’t make any sense, just like the baby has no understanding of different words. It’s only after the model has started getting trained, the word vectors/embeddings start to capture the meaning of the words, just like the baby hears and learns different words.\""
      ],
      "metadata": {
        "id": "o7OsPVqbrbqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "df8-lpXbtHe2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]"
      ],
      "metadata": {
        "id": "W2EvpsPVvgmW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unigram transformation"
      ],
      "metadata": {
        "id": "UfvwcF5Uz_-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "from typing import List\n",
        "\n",
        "def ngrams_transform(document: List[str],\n",
        "                     n_gram: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    N-grams transformations for a given text\n",
        "\n",
        "    Args:\n",
        "    document (List[str]) -- The document to-be-processed\n",
        "    n_gram   (int)       -- Number of grams\n",
        "\n",
        "    Returns:\n",
        "    A list of string after n-grams processed\n",
        "    \"\"\"\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "\n",
        "     # Use nltk.ngrams to generate tuples of n-grams\n",
        "    # ngrams() returns an iterator, so we need to process it\n",
        "    ngram_tuples = ngrams(document, n_gram)\n",
        "\n",
        "    # Convert each tuple into a space-separated string\n",
        "    # Use a list comprehension for conciseness\n",
        "    ngram_list = [\" \".join(gram) for gram in ngram_tuples]\n",
        "\n",
        "    return ngram_list\n",
        "\n",
        "    ### END YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "vPNwu0lWymvJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_grams_list = ngrams_transform(corpus,\n",
        "                                n_gram=1)\n",
        "n_grams_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKPIUAu_-gX5",
        "outputId": "f016323c-2737-42e5-fc57-3a26b69c1e88"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This is the first document.',\n",
              " 'This document is the second document.',\n",
              " 'And this is the third one.',\n",
              " 'Is this the first document?']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Integer label for the given corpus\n",
        "label_encoder = LabelEncoder()\n",
        "corpus_vector = label_encoder.fit_transform(np.array(n_grams_list))\n",
        "\n",
        "# Tensorize the input vector\n",
        "example_text_tensor = torch.Tensor(corpus_vector).to(dtype=torch.long)\n",
        "print(f\"Example text tensor: {example_text_tensor}\")\n",
        "print(f\"Shape of example text tensor: {example_text_tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w99wG6cn0Dap",
        "outputId": "e0355f0b-b548-499b-aa1f-e52550b98f05"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example text tensor: tensor([3, 2, 0, 1])\n",
            "Shape of example text tensor: torch.Size([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an example for embedding function to map from a word dimension to a lower dimensional space"
      ],
      "metadata": {
        "id": "0uVmzQtLvs66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_vocab = 22 # number of vocabulary\n",
        "num_dimension = 50 # dimensional embeddings\n",
        "\n",
        "# Declare the mapping function\n",
        "example_embedding_function = nn.Embedding(num_vocab, num_dimension)"
      ],
      "metadata": {
        "id": "OJTdk0_drc0L"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_output_tensor = example_embedding_function(example_text_tensor)\n",
        "print(f\"Embedding shape: {example_output_tensor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmTjA3-4uGKG",
        "outputId": "79d68ac9-6f7d-43d9-cd7e-591592ef566e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: torch.Size([4, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec\n",
        "\n",
        "\n",
        "* Word2vec is a **class of models** that represents a word in a large text corpus as a vector in n-dimensional space(or n-dimensional feature space) bringing similar words closer to each other.\n",
        "\n",
        "\n",
        "\n",
        "* Word2vec is a simple yet popular model to construct representating embedding for words from a representation space to a much lower dimensional space (compared to the respective number of words in a dictionary).\n",
        "\n",
        "\n",
        "\n",
        "* Word2Vec has two neural network-based variants, which are:\n",
        "\n",
        "    * Continuous Bag of Words (CBOW)\n",
        "    * Skip-gram.\n",
        "![](https://kavita-ganesan.com/wp-content/uploads/skipgram-vs-cbow-continuous-bag-of-words-word2vec-word-representation-2048x1075.png)\n"
      ],
      "metadata": {
        "id": "oElUUcX2ZyDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous Bag of words (CBOW)\n",
        "\n",
        "* The Continuous Bag-of-Words model (CBOW) is frequently used in NLP deep learning. It is a model that tries to predict words given the context of a few words before and a few words after the target word. This is distinct from language modeling, since CBOW is not sequential and does not have to be probabilistic. Typically, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings. It almost always helps performance a couple of percent.\n",
        "\n",
        "* CBOW is modelled as follows:\n",
        "    * Given a target word $w_i$ and an $N$ context window on each side, $w_{i-1}, \\cdots, w_{i-N}$ and $w_{i+1},\\cdots, w_{i+N}$, referring to all context words collectively as $C$.\n",
        "\n",
        "    * CBOW tries to minimize the objective function:\n",
        "\n",
        "$$\n",
        "-\\log p(w_i|C) = -\\log\\text{Softmax}\\left(A\\left(\\sum_{w\\in C}q_w\\right)+b\\right)\n",
        "$$\n",
        "\n",
        "where $q_w$ is the embedding of word $w$."
      ],
      "metadata": {
        "id": "KwAVXC0V8vWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N = 2 according to the definition\n",
        "CONTEXT_SIZE = 2\n",
        "\n",
        "corpus = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\"\n",
        "\n",
        "corpus = corpus.split()\n",
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSvo8dncDUvv",
        "outputId": "e521f180-da81-446a-c3ea-851f9e75c0b9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an integer mapping"
      ],
      "metadata": {
        "id": "_3MOE0WmFBd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(corpus)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Integer word mapping\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "word_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4eHuiF8yOj",
        "outputId": "d65ccf2c-d093-4ce6-c73b-9b83cf661eb9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'about': 0,\n",
              " 'process.': 1,\n",
              " 'direct': 2,\n",
              " 'processes': 3,\n",
              " 'beings': 4,\n",
              " 'abstract': 5,\n",
              " 'rules': 6,\n",
              " 'idea': 7,\n",
              " 'computers.': 8,\n",
              " 'our': 9,\n",
              " 'Computational': 10,\n",
              " 'that': 11,\n",
              " 'with': 12,\n",
              " 'spirits': 13,\n",
              " 'by': 14,\n",
              " 'the': 15,\n",
              " 'effect,': 16,\n",
              " 'a': 17,\n",
              " 'computational': 18,\n",
              " 'conjure': 19,\n",
              " 'called': 20,\n",
              " 'We': 21,\n",
              " 'study': 22,\n",
              " 'As': 23,\n",
              " 'evolution': 24,\n",
              " 'pattern': 25,\n",
              " 'processes.': 26,\n",
              " 'programs': 27,\n",
              " 'other': 28,\n",
              " 'directed': 29,\n",
              " 'is': 30,\n",
              " 'to': 31,\n",
              " 'evolve,': 32,\n",
              " 'are': 33,\n",
              " 'The': 34,\n",
              " 'things': 35,\n",
              " 'People': 36,\n",
              " 'data.': 37,\n",
              " 'spells.': 38,\n",
              " 'manipulate': 39,\n",
              " 'create': 40,\n",
              " 'In': 41,\n",
              " 'program.': 42,\n",
              " 'of': 43,\n",
              " 'process': 44,\n",
              " 'inhabit': 45,\n",
              " 'they': 46,\n",
              " 'computer': 47,\n",
              " 'we': 48}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build context according to the given corpus"
      ],
      "metadata": {
        "id": "sqrPWRsTE5zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "for i in range(CONTEXT_SIZE, len(corpus) - CONTEXT_SIZE):\n",
        "    context = (\n",
        "        [corpus[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
        "        + [corpus[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
        "    )\n",
        "    target = corpus[i]\n",
        "    data.append((context, target))\n",
        "\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgBAHKy5CoJ7",
        "outputId": "cb1b84f8-ab44-4702-8ab0-9db6837c6ced"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['are', 'We', 'to', 'study'], 'about'),\n",
              " (['about', 'are', 'study', 'the'], 'to'),\n",
              " (['to', 'about', 'the', 'idea'], 'study'),\n",
              " (['study', 'to', 'idea', 'of'], 'the'),\n",
              " (['the', 'study', 'of', 'a'], 'idea'),\n",
              " (['idea', 'the', 'a', 'computational'], 'of'),\n",
              " (['of', 'idea', 'computational', 'process.'], 'a'),\n",
              " (['a', 'of', 'process.', 'Computational'], 'computational'),\n",
              " (['computational', 'a', 'Computational', 'processes'], 'process.'),\n",
              " (['process.', 'computational', 'processes', 'are'], 'Computational'),\n",
              " (['Computational', 'process.', 'are', 'abstract'], 'processes'),\n",
              " (['processes', 'Computational', 'abstract', 'beings'], 'are'),\n",
              " (['are', 'processes', 'beings', 'that'], 'abstract'),\n",
              " (['abstract', 'are', 'that', 'inhabit'], 'beings'),\n",
              " (['beings', 'abstract', 'inhabit', 'computers.'], 'that'),\n",
              " (['that', 'beings', 'computers.', 'As'], 'inhabit'),\n",
              " (['inhabit', 'that', 'As', 'they'], 'computers.'),\n",
              " (['computers.', 'inhabit', 'they', 'evolve,'], 'As'),\n",
              " (['As', 'computers.', 'evolve,', 'processes'], 'they'),\n",
              " (['they', 'As', 'processes', 'manipulate'], 'evolve,'),\n",
              " (['evolve,', 'they', 'manipulate', 'other'], 'processes'),\n",
              " (['processes', 'evolve,', 'other', 'abstract'], 'manipulate'),\n",
              " (['manipulate', 'processes', 'abstract', 'things'], 'other'),\n",
              " (['other', 'manipulate', 'things', 'called'], 'abstract'),\n",
              " (['abstract', 'other', 'called', 'data.'], 'things'),\n",
              " (['things', 'abstract', 'data.', 'The'], 'called'),\n",
              " (['called', 'things', 'The', 'evolution'], 'data.'),\n",
              " (['data.', 'called', 'evolution', 'of'], 'The'),\n",
              " (['The', 'data.', 'of', 'a'], 'evolution'),\n",
              " (['evolution', 'The', 'a', 'process'], 'of'),\n",
              " (['of', 'evolution', 'process', 'is'], 'a'),\n",
              " (['a', 'of', 'is', 'directed'], 'process'),\n",
              " (['process', 'a', 'directed', 'by'], 'is'),\n",
              " (['is', 'process', 'by', 'a'], 'directed'),\n",
              " (['directed', 'is', 'a', 'pattern'], 'by'),\n",
              " (['by', 'directed', 'pattern', 'of'], 'a'),\n",
              " (['a', 'by', 'of', 'rules'], 'pattern'),\n",
              " (['pattern', 'a', 'rules', 'called'], 'of'),\n",
              " (['of', 'pattern', 'called', 'a'], 'rules'),\n",
              " (['rules', 'of', 'a', 'program.'], 'called'),\n",
              " (['called', 'rules', 'program.', 'People'], 'a'),\n",
              " (['a', 'called', 'People', 'create'], 'program.'),\n",
              " (['program.', 'a', 'create', 'programs'], 'People'),\n",
              " (['People', 'program.', 'programs', 'to'], 'create'),\n",
              " (['create', 'People', 'to', 'direct'], 'programs'),\n",
              " (['programs', 'create', 'direct', 'processes.'], 'to'),\n",
              " (['to', 'programs', 'processes.', 'In'], 'direct'),\n",
              " (['direct', 'to', 'In', 'effect,'], 'processes.'),\n",
              " (['processes.', 'direct', 'effect,', 'we'], 'In'),\n",
              " (['In', 'processes.', 'we', 'conjure'], 'effect,'),\n",
              " (['effect,', 'In', 'conjure', 'the'], 'we'),\n",
              " (['we', 'effect,', 'the', 'spirits'], 'conjure'),\n",
              " (['conjure', 'we', 'spirits', 'of'], 'the'),\n",
              " (['the', 'conjure', 'of', 'the'], 'spirits'),\n",
              " (['spirits', 'the', 'the', 'computer'], 'of'),\n",
              " (['of', 'spirits', 'computer', 'with'], 'the'),\n",
              " (['the', 'of', 'with', 'our'], 'computer'),\n",
              " (['computer', 'the', 'our', 'spells.'], 'with')]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 2\n",
        "Name at least 2 limitations at this context construction step? Explain your answers."
      ],
      "metadata": {
        "id": "EbFTMNAGWygb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorize context"
      ],
      "metadata": {
        "id": "RYMlC5FSHSqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_context_vector(context: List[str],\n",
        "                        word_to_idx: dict) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Function to map a word context vector into a torch tensor\n",
        "\n",
        "    Args:\n",
        "    context (List[str]) -- A context (including individual n-grams tokens)\n",
        "    word_to_idx (dict)  -- A functionto map a word into its respective integer\n",
        "\n",
        "    Returns:\n",
        "    A pytorch tensor including a list of mapped word\n",
        "\n",
        "    Example:\n",
        "    ['are', 'We', 'to', 'study'] --> tensor([40, 22, 27, 47])\n",
        "    \"\"\"\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "\n",
        "    # 1. Look up the index for each word in the context using the word_to_idx dictionary.\n",
        "    #    A list comprehension is a concise way to do this.\n",
        "    idxs = [word_to_idx[word] for word in context]\n",
        "\n",
        "    # 2. Convert the list of indices into a PyTorch tensor.\n",
        "    #    Specify dtype=torch.long as indices are typically represented as long integers.\n",
        "    context_vector = torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "    return context_vector\n",
        "\n",
        "    ### END YOUR CODE HERE ###"
      ],
      "metadata": {
        "id": "9aRCDPmYEnMg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functional test\n",
        "print(\"Example sample: \", data[0][0])\n",
        "make_context_vector(data[0][0], word_to_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmf-xZkEFOfZ",
        "outputId": "5cee0d23-1f06-4313-d569-dae86736c012"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example sample:  ['are', 'We', 'to', 'study']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([33, 21, 31, 22])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBOW model implementation"
      ],
      "metadata": {
        "id": "-h_4P68vHVOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 embed_dim: int) -> None:\n",
        "        \"\"\"\n",
        "        Model constructor\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.linear_layer = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        # Neural weight initialization\n",
        "        nn.init.xavier_normal_(self.embedding_layer.weight)\n",
        "        nn.init.xavier_normal_(self.linear_layer.weight)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Function to conduct forward passing\n",
        "        \"\"\"\n",
        "        embedding = self.embedding_layer(inputs)\n",
        "        embedding = torch.sum(embedding, dim=1)\n",
        "        output = self.linear_layer(embedding)\n",
        "        output_softmax = F.log_softmax(output, dim=1)\n",
        "        return output_softmax"
      ],
      "metadata": {
        "id": "ViLponY1HZoD"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_model = CBOW(vocab_size=vocab_size,\n",
        "                  embed_dim=10)\n",
        "\n",
        "# Enable gradient for model training\n",
        "cbow_model.train()\n",
        "cbow_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_vTUt8aJiEn",
        "outputId": "3848ffae-efa2-4288-f64b-ae0deab4aac7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CBOW(\n",
              "  (embedding_layer): Embedding(49, 10)\n",
              "  (linear_layer): Linear(in_features=10, out_features=49, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "s9aXrzRCKQel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameters and training configuration"
      ],
      "metadata": {
        "id": "L5tYJoeKMmuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs: int = 5\n",
        "learning_rate: float = 5e-2\n",
        "optimizer: torch.optim = torch.optim.Adam(cbow_model.parameters(),\n",
        "                                          lr=learning_rate)\n",
        "\n",
        "loss_function = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "UxbOAYCDLlrd"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training phase"
      ],
      "metadata": {
        "id": "vgn6aHDsWk2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"#Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    # Construct input and target tensor\n",
        "    input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n",
        "    input_vector = input_vector.unsqueeze(0)\n",
        "    target_vector = target_vector.unsqueeze(0)\n",
        "\n",
        "    # Join whole data into 1 tensor set\n",
        "    for idx in range(1, len(data)):\n",
        "        input_tensor = torch.tensor(make_context_vector(data[idx][0], word_to_idx)).unsqueeze(0)\n",
        "        target_tensor = torch.tensor(word_to_idx[data[idx][1]]).unsqueeze(0)\n",
        "        torch.cat((input_vector, input_tensor), 0)\n",
        "        torch.cat((target_vector, target_tensor), 0)\n",
        "\n",
        "    # Zero out the gradients from the old instance to avoid tensor accumulation\n",
        "    cbow_model.zero_grad()\n",
        "\n",
        "    # Forward passing\n",
        "    log_probabilities = cbow_model(input_vector)\n",
        "\n",
        "    # Evaluate loss\n",
        "    loss = loss_function(log_probabilities, target_vector)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the gradient according to the optimization algorithm\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get loss values\n",
        "    epoch_loss = loss.item()\n",
        "    print(\"Loss:\", epoch_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bv0CVQZKS7W",
        "outputId": "21962ca8-3286-46cc-acce-0d1263dc3d9d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Epoch 1/5\n",
            "Loss: 3.704463243484497\n",
            "#Epoch 2/5\n",
            "Loss: 3.2461307048797607\n",
            "#Epoch 3/5\n",
            "Loss: 2.7708699703216553\n",
            "#Epoch 4/5\n",
            "Loss: 2.1763405799865723\n",
            "#Epoch 5/5\n",
            "Loss: 1.4540271759033203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-fe12cc1eff7c>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_vector, target_vector = torch.tensor(make_context_vector(data[0][0], word_to_idx)), torch.tensor(word_to_idx[data[0][1]])\n",
            "<ipython-input-55-fe12cc1eff7c>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_tensor = torch.tensor(make_context_vector(data[idx][0], word_to_idx)).unsqueeze(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inference"
      ],
      "metadata": {
        "id": "7DrF_htiSXmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # No gradient update in inference\n",
        "    context = ['In', 'processes.', 'we', 'conjure']\n",
        "\n",
        "    # Vectorize input from text to numeric type\n",
        "    input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n",
        "\n",
        "    # Model makes prediction\n",
        "    output_tensor = cbow_model(input_tensor)\n",
        "\n",
        "    # Get the item id with the highest probability\n",
        "    prediction = torch.argmax(output_tensor).detach().tolist()\n",
        "\n",
        "    # Query the respective word from the given item id\n",
        "    key_list = list(word_to_idx.keys())\n",
        "    prediction = key_list[prediction]\n",
        "\n",
        "    print(\"Context:\", context)\n",
        "    print(\"Prediction:\", prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3SBZo4LSDP5",
        "outputId": "63cf296e-e972-4cb0-bfe6-6b8c78e9c63c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: ['In', 'processes.', 'we', 'conjure']\n",
            "Prediction: about\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-149b12a03e6d>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip-gram\n",
        "\n",
        "<center>\n",
        "<img src=\"https://machinelearningcoban.com/tabml_book/_images/word2vec2.png\">\n",
        "</center>\n",
        "\n",
        "- Skip gram is based on the distributional hypothesis where words with similar distribution is considered to have similar meanings. Researchers of skip gram suggested a model with less parameters along with the novel methods to make optimization step more efficient.\n",
        "\n",
        "- Vanilla SkipGram model:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://d3i71xaburhd42.cloudfront.net/a1d083c872e848787cb572a73d97f2c24947a374/5-Figure1-1.png\" scale=70%>\n",
        "</center>\n",
        "\n",
        "- Main idea is to optimize model so that if it is queried with a word, it should correctly guess all the context (context = 2 in the figure) words. That is,\n",
        "$$\n",
        "y=\\sigma(Ux)\n",
        "$$\n",
        "    - where $x$, $y$ are one-hot encoded word vector, $U$ is the embedding matrix, and $\\sigma(\\cdot)$ is the softmax function.\n",
        "\n",
        "With the same dataset, training set for skip gram can be much larger than that of NPLM since it can have $2c$ samples $\\left(w_t:w_{t-c}, ...,w_t:w_{t-1},w_t:w_{t+1},...,w_{t+c}\\right)$ while other n-gram based models have one $\\left((w_{t-c},...w_{t-1},w_{t+1},...,w_{t+c}):w_t\\right)$."
      ],
      "metadata": {
        "id": "X9FwHWmGaM3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\""
      ],
      "metadata": {
        "id": "b9XRCWEvEOWC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_context_vector(context: List[str],\n",
        "                        word_to_idx: dict) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Function to map a word context vector into a torch tensor\n",
        "\n",
        "    Args:\n",
        "    context (List[str]) -- A context (including individual n-grams tokens)\n",
        "    word_to_idx (dict)  -- A functionto map a word into its respective integer\n",
        "\n",
        "    Returns:\n",
        "    A pytorch tensor including a list of mapped word\n",
        "\n",
        "    Example:\n",
        "    ['are', 'We', 'to', 'study'] --> tensor([40, 22, 27, 47])\n",
        "    \"\"\"\n",
        "    # Map words to indices, handling potential unknown words (assigning a default index, e.g., 0 for <UNK>)\n",
        "    idxs = [word_to_idx.get(w, word_to_idx.get(\"<UNK>\", 0)) for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ],
      "metadata": {
        "id": "c8Mvb0cIkfqK"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 embed_dim: int) -> None:\n",
        "        \"\"\"\n",
        "        Model construction\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        ### START YOUR CODE HERE ###\n",
        "\n",
        "        # Declare embedding function u and v\n",
        "        # with given vocab size and embed dim using nn.Embedding\n",
        "        # v_embedding_layer: Represents the embeddings for the center words (input words)\n",
        "        self.v_embedding_layer = nn.Embedding(self.vocab_size, self.embed_dim)\n",
        "        # u_embedding_layer: Represents the embeddings for the context words (output words)\n",
        "        self.u_embedding_layer = nn.Embedding(self.vocab_size, self.embed_dim)\n",
        "\n",
        "        # Network weight initialization with Xavier uniform initialization\n",
        "        # Ensure 'init' is imported: import torch.nn.init as init\n",
        "        nn.init.xavier_uniform_(self.v_embedding_layer.weight)\n",
        "        nn.init.xavier_uniform_(self.u_embedding_layer.weight)\n",
        "\n",
        "        ### END YOUR CODE HERE ###\n",
        "\n",
        "    def forward(self, center_words, context):\n",
        "        \"\"\"\n",
        "        Function to perform forward passing\n",
        "        \"\"\"\n",
        "        v_embedding = self.v_embedding_layer(center_words)\n",
        "        u_embedding = self.u_embedding_layer(context)\n",
        "\n",
        "        score = torch.mul(v_embedding, u_embedding)\n",
        "        score = torch.sum(score, dim=1)\n",
        "        log_score = F.logsigmoid(score)\n",
        "        return log_score"
      ],
      "metadata": {
        "id": "S1dwgXNxZ_Lv"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_model = SkipGramModel(vocab_size=vocab_size,\n",
        "                               embed_dim=128)\n",
        "\n",
        "skipgram_model.train()\n",
        "skipgram_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4ls_wI-7nS1",
        "outputId": "9037683c-bd9f-4e66-f7da-3f6e89099212"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SkipGramModel(\n",
              "  (v_embedding_layer): Embedding(49, 128)\n",
              "  (u_embedding_layer): Embedding(49, 128)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare training data to match the format of SkipGram model"
      ],
      "metadata": {
        "id": "oiBCaJJJCMr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gather_training_data(corpus,\n",
        "                         word_to_idx: dict,\n",
        "                         context_size: int):\n",
        "    \"\"\"\n",
        "    This function is to transform the given corpus\n",
        "    into the correct format for SkipGram to serve as its input\n",
        "    \"\"\"\n",
        "\n",
        "    training_data = []\n",
        "    all_vocab_indices = list(range(len(word_to_idx)))\n",
        "\n",
        "    split_text = corpus.split('\\n')\n",
        "\n",
        "    # For each sentence\n",
        "    for sentence in split_text:\n",
        "        indices = []\n",
        "        indices = [word_to_idx[word] for word in sentence.split(' ')]\n",
        "\n",
        "        # For each word treated as center word\n",
        "        for center_word_pos in range(len(indices)):\n",
        "\n",
        "            # For each window  position\n",
        "            for w in range(-context_size, context_size+1):\n",
        "                context_word_pos = center_word_pos + w\n",
        "\n",
        "                # Make sure we dont jump out of the sentence\n",
        "                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
        "                    continue\n",
        "\n",
        "                context_word_idx = indices[context_word_pos]\n",
        "                center_word_idx  = indices[center_word_pos]\n",
        "\n",
        "                # Same words might be present in the close vicinity of each other. we want to avoid such cases\n",
        "                if center_word_idx == context_word_idx:\n",
        "                    continue\n",
        "\n",
        "                training_data.append([center_word_idx, context_word_idx])\n",
        "\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "5YlRPTuaCQBS"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = gather_training_data(corpus,\n",
        "                                     word_to_idx,\n",
        "                                     context_size=2)\n",
        "training_data = torch.tensor(training_data).to(dtype=torch.long)\n",
        "training_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVjk1oJkDPXq",
        "outputId": "7adce366-cec7-4ebd-8bdd-f3dd4757fc7d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([212, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcnABem16t8w",
        "outputId": "e2c70b79-d547-469e-d0b9-54a79be9520d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[21, 33],\n",
              "        [21,  0],\n",
              "        [33, 21],\n",
              "        [33,  0],\n",
              "        [33, 31],\n",
              "        [ 0, 21],\n",
              "        [ 0, 33],\n",
              "        [ 0, 31],\n",
              "        [ 0, 22],\n",
              "        [31, 33],\n",
              "        [31,  0],\n",
              "        [31, 22],\n",
              "        [31, 15],\n",
              "        [22,  0],\n",
              "        [22, 31],\n",
              "        [22, 15],\n",
              "        [22,  7],\n",
              "        [15, 31],\n",
              "        [15, 22],\n",
              "        [15,  7],\n",
              "        [15, 43],\n",
              "        [ 7, 22],\n",
              "        [ 7, 15],\n",
              "        [ 7, 43],\n",
              "        [ 7, 17],\n",
              "        [43, 15],\n",
              "        [43,  7],\n",
              "        [43, 17],\n",
              "        [43, 18],\n",
              "        [17,  7],\n",
              "        [17, 43],\n",
              "        [17, 18],\n",
              "        [17,  1],\n",
              "        [18, 43],\n",
              "        [18, 17],\n",
              "        [18,  1],\n",
              "        [ 1, 17],\n",
              "        [ 1, 18],\n",
              "        [10,  3],\n",
              "        [10, 33],\n",
              "        [ 3, 10],\n",
              "        [ 3, 33],\n",
              "        [ 3,  5],\n",
              "        [33, 10],\n",
              "        [33,  3],\n",
              "        [33,  5],\n",
              "        [33,  4],\n",
              "        [ 5,  3],\n",
              "        [ 5, 33],\n",
              "        [ 5,  4],\n",
              "        [ 5, 11],\n",
              "        [ 4, 33],\n",
              "        [ 4,  5],\n",
              "        [ 4, 11],\n",
              "        [ 4, 45],\n",
              "        [11,  5],\n",
              "        [11,  4],\n",
              "        [11, 45],\n",
              "        [11,  8],\n",
              "        [45,  4],\n",
              "        [45, 11],\n",
              "        [45,  8],\n",
              "        [ 8, 11],\n",
              "        [ 8, 45],\n",
              "        [23, 46],\n",
              "        [23, 32],\n",
              "        [46, 23],\n",
              "        [46, 32],\n",
              "        [46,  3],\n",
              "        [32, 23],\n",
              "        [32, 46],\n",
              "        [32,  3],\n",
              "        [32, 39],\n",
              "        [ 3, 46],\n",
              "        [ 3, 32],\n",
              "        [ 3, 39],\n",
              "        [ 3, 28],\n",
              "        [39, 32],\n",
              "        [39,  3],\n",
              "        [39, 28],\n",
              "        [39,  5],\n",
              "        [28,  3],\n",
              "        [28, 39],\n",
              "        [28,  5],\n",
              "        [28, 35],\n",
              "        [ 5, 39],\n",
              "        [ 5, 28],\n",
              "        [ 5, 35],\n",
              "        [ 5, 20],\n",
              "        [35, 28],\n",
              "        [35,  5],\n",
              "        [35, 20],\n",
              "        [35, 37],\n",
              "        [20,  5],\n",
              "        [20, 35],\n",
              "        [20, 37],\n",
              "        [37, 35],\n",
              "        [37, 20],\n",
              "        [34, 24],\n",
              "        [34, 43],\n",
              "        [24, 34],\n",
              "        [24, 43],\n",
              "        [24, 17],\n",
              "        [43, 34],\n",
              "        [43, 24],\n",
              "        [43, 17],\n",
              "        [43, 44],\n",
              "        [17, 24],\n",
              "        [17, 43],\n",
              "        [17, 44],\n",
              "        [17, 30],\n",
              "        [44, 43],\n",
              "        [44, 17],\n",
              "        [44, 30],\n",
              "        [44, 29],\n",
              "        [30, 17],\n",
              "        [30, 44],\n",
              "        [30, 29],\n",
              "        [30, 14],\n",
              "        [29, 44],\n",
              "        [29, 30],\n",
              "        [29, 14],\n",
              "        [29, 17],\n",
              "        [14, 30],\n",
              "        [14, 29],\n",
              "        [14, 17],\n",
              "        [14, 25],\n",
              "        [17, 29],\n",
              "        [17, 14],\n",
              "        [17, 25],\n",
              "        [17, 43],\n",
              "        [25, 14],\n",
              "        [25, 17],\n",
              "        [25, 43],\n",
              "        [25,  6],\n",
              "        [43, 17],\n",
              "        [43, 25],\n",
              "        [43,  6],\n",
              "        [ 6, 25],\n",
              "        [ 6, 43],\n",
              "        [20, 17],\n",
              "        [20, 42],\n",
              "        [17, 20],\n",
              "        [17, 42],\n",
              "        [17, 36],\n",
              "        [42, 20],\n",
              "        [42, 17],\n",
              "        [42, 36],\n",
              "        [42, 40],\n",
              "        [36, 17],\n",
              "        [36, 42],\n",
              "        [36, 40],\n",
              "        [36, 27],\n",
              "        [40, 42],\n",
              "        [40, 36],\n",
              "        [40, 27],\n",
              "        [40, 31],\n",
              "        [27, 36],\n",
              "        [27, 40],\n",
              "        [27, 31],\n",
              "        [27,  2],\n",
              "        [31, 40],\n",
              "        [31, 27],\n",
              "        [31,  2],\n",
              "        [31, 26],\n",
              "        [ 2, 27],\n",
              "        [ 2, 31],\n",
              "        [ 2, 26],\n",
              "        [ 2, 41],\n",
              "        [26, 31],\n",
              "        [26,  2],\n",
              "        [26, 41],\n",
              "        [26, 16],\n",
              "        [41,  2],\n",
              "        [41, 26],\n",
              "        [41, 16],\n",
              "        [16, 26],\n",
              "        [16, 41],\n",
              "        [48, 19],\n",
              "        [48, 15],\n",
              "        [19, 48],\n",
              "        [19, 15],\n",
              "        [19, 13],\n",
              "        [15, 48],\n",
              "        [15, 19],\n",
              "        [15, 13],\n",
              "        [15, 43],\n",
              "        [13, 19],\n",
              "        [13, 15],\n",
              "        [13, 43],\n",
              "        [13, 15],\n",
              "        [43, 15],\n",
              "        [43, 13],\n",
              "        [43, 15],\n",
              "        [43, 47],\n",
              "        [15, 13],\n",
              "        [15, 43],\n",
              "        [15, 47],\n",
              "        [15, 12],\n",
              "        [47, 43],\n",
              "        [47, 15],\n",
              "        [47, 12],\n",
              "        [47,  9],\n",
              "        [12, 15],\n",
              "        [12, 47],\n",
              "        [12,  9],\n",
              "        [12, 38],\n",
              "        [ 9, 47],\n",
              "        [ 9, 12],\n",
              "        [ 9, 38],\n",
              "        [38, 12],\n",
              "        [38,  9]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparamters and training configuration"
      ],
      "metadata": {
        "id": "tWEIXRlg8zmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs: int = 200\n",
        "learning_rate: float = 5e-1\n",
        "optimizer: torch.optim = torch.optim.SGD(skipgram_model.parameters(),\n",
        "                                          lr=learning_rate)"
      ],
      "metadata": {
        "id": "wIgEYDKz8Sbu"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector = training_data[:, 0]\n",
        "# Accessing elements of the tensor directly as indices for word_to_idx\n",
        "target_vector = training_data[:, 1]"
      ],
      "metadata": {
        "id": "Mx4u2RAGrI6z"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVQRYBmnHKRB",
        "outputId": "53b95be5-0dfc-411f-cd39-737297625d59"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([21, 21, 33, 33, 33,  0,  0,  0,  0, 31, 31, 31, 31, 22, 22, 22, 22, 15,\n",
              "        15, 15, 15,  7,  7,  7,  7, 43, 43, 43, 43, 17, 17, 17, 17, 18, 18, 18,\n",
              "         1,  1, 10, 10,  3,  3,  3, 33, 33, 33, 33,  5,  5,  5,  5,  4,  4,  4,\n",
              "         4, 11, 11, 11, 11, 45, 45, 45,  8,  8, 23, 23, 46, 46, 46, 32, 32, 32,\n",
              "        32,  3,  3,  3,  3, 39, 39, 39, 39, 28, 28, 28, 28,  5,  5,  5,  5, 35,\n",
              "        35, 35, 35, 20, 20, 20, 37, 37, 34, 34, 24, 24, 24, 43, 43, 43, 43, 17,\n",
              "        17, 17, 17, 44, 44, 44, 44, 30, 30, 30, 30, 29, 29, 29, 29, 14, 14, 14,\n",
              "        14, 17, 17, 17, 17, 25, 25, 25, 25, 43, 43, 43,  6,  6, 20, 20, 17, 17,\n",
              "        17, 42, 42, 42, 42, 36, 36, 36, 36, 40, 40, 40, 40, 27, 27, 27, 27, 31,\n",
              "        31, 31, 31,  2,  2,  2,  2, 26, 26, 26, 26, 41, 41, 41, 16, 16, 48, 48,\n",
              "        19, 19, 19, 15, 15, 15, 15, 13, 13, 13, 13, 43, 43, 43, 43, 15, 15, 15,\n",
              "        15, 47, 47, 47, 47, 12, 12, 12, 12,  9,  9,  9, 38, 38])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training phase"
      ],
      "metadata": {
        "id": "NmG0m3jK84EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs + 1):\n",
        "    \"\"\"\n",
        "    Adapt the given CBOW training code for SkipGram\n",
        "    Following by the instruction comments, or you could do it on your own ;)\n",
        "    \"\"\"\n",
        "    # Construct input and target tensor\n",
        "    inputs = training_data[:, 0]  # Center words\n",
        "    targets = training_data[:, 1]  # Context words\n",
        "\n",
        "    skipgram_model.zero_grad()\n",
        "\n",
        "    # Zero out the gradients from the old instance to avoid tensor accumulation\n",
        "\n",
        "\n",
        "    # Forward passing\n",
        "    logsoftmax_prediction = skipgram_model(inputs, targets)\n",
        "\n",
        "    # Evaluate loss (Negative log likelihood)\n",
        "    loss = torch.mean(-1 * logsoftmax_prediction)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the gradient according to the optimization algorithm\n",
        "    optimizer.step()\n",
        "\n",
        "    # Get loss values\n",
        "    epoch_loss = loss.item()\n",
        "\n",
        "    # Log result\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"#Epoch {epoch}/{num_epochs}\")\n",
        "        print(\"Loss:\", epoch_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhGy9vBq85PX",
        "outputId": "a3070566-2ce3-48c2-805b-623813abaac6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Epoch 0/200\n",
            "Loss: 0.6916677951812744\n",
            "#Epoch 50/200\n",
            "Loss: 0.606799840927124\n",
            "#Epoch 100/200\n",
            "Loss: 0.5255119800567627\n",
            "#Epoch 150/200\n",
            "Loss: 0.44386813044548035\n",
            "#Epoch 200/200\n",
            "Loss: 0.3657078146934509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "-X8DX0BfFDsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n",
        "print(input_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTU2PtDhIJdN",
        "outputId": "4a50b3ee-55d8-4b86-e265-4203655d51df"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[41, 26, 48, 19]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-68-775e9234e517>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_tensor = torch.tensor(make_context_vector(context, word_to_idx)).unsqueeze(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    context = ['we']\n",
        "\n",
        "    ### START YOUR CODE HERE ###\n",
        "    # Based on the given inference code in the previous section, training code and the context\n",
        "    # Implement the inference flow from the given context to an output word\n",
        "      # Convert the input word to its index\n",
        "    input_word_idx = torch.tensor([word_to_idx[context[0]]])\n",
        "\n",
        "    # Get the embedding for the input word\n",
        "    input_word_embedding = skipgram_model.v_embedding_layer(input_word_idx)\n",
        "\n",
        "    # Calculate similarity scores with all words in the vocabulary\n",
        "    all_word_embeddings = skipgram_model.u_embedding_layer.weight\n",
        "    similarity_scores = torch.matmul(input_word_embedding, all_word_embeddings.T)\n",
        "\n",
        "    # Get the word index with the highest score (excluding the input word)\n",
        "    scores = similarity_scores.flatten()\n",
        "    input_word_idx_value = input_word_idx.item()\n",
        "    scores[input_word_idx_value] = float('-inf')  # Exclude the input word\n",
        "\n",
        "    # Get the highest scoring word index\n",
        "    best_idx = torch.argmax(scores).item()\n",
        "\n",
        "    # Convert the index back to a word\n",
        "    key_list = list(word_to_idx.keys())\n",
        "    prediction = key_list[best_idx]\n",
        "\n",
        "    ### END YOUR CODE HERE ###\n",
        "    print(\"Context:\", context)\n",
        "    print(\"Prediction:\", prediction)"
      ],
      "metadata": {
        "id": "U2EcHAqVFEzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3bbfd2-a19f-425e-a4be-1eacfb172ad1"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: ['we']\n",
            "Prediction: the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3\n",
        "What are the differences between CBOW and Skip-gram?"
      ],
      "metadata": {
        "id": "6sg4EaBpw-De"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Core Differences:**  \n",
        "\n",
        "\n",
        " **1. CBOW (Continuous Bag-of-Words):**  \n",
        "   * **Objective:** Predict the **target word** based on surrounding **context words**.  \n",
        "   * **How it works:** The model takes vectors of context words within a window, typically averaging (or summing) them, and uses the combined vector to predict the middle word (target word).  \n",
        "   * **Example:** Given the context \"the cat ___ on the mat,\" CBOW tries to predict the missing word, e.g., \"sits.\"  \n",
        "   * **Input:** Multiple context word vectors.  \n",
        "   * **Output:** A single target word vector.  \n",
        "\n",
        " **2. Skip-gram:**  \n",
        "   * **Objective:** Predict **context words** around a given **target word**.  \n",
        "   * **How it works:** The model takes the target word’s vector and uses it to predict different words that are likely to appear in its surrounding window.  \n",
        "   * **Example:** Given the target word \"sits,\" Skip-gram tries to predict context words such as \"the,\" \"cat,\" \"on,\" \"the,\" \"mat.\"  \n",
        "   * **Input:** A single target word vector.  \n",
        "   * **Output:** Multiple context word vectors.  \n",
        "\n",
        " **Detailed Comparison Table:**  \n",
        "\n",
        "| **Criteria**            | **CBOW (Continuous Bag-of-Words)**                | **Skip-gram**                               |\n",
        "|------------------------|------------------------------------------------|---------------------------------------------|\n",
        "| **Main objective**     | Predicts the target word from context words.   | Predicts context words from a target word. |\n",
        "| **Input**             | Vectors of multiple context words.             | Vector of a single target word.            |\n",
        "| **Output**            | Vector of a single target word.                | Vectors of multiple context words.         |\n",
        "| **Training speed**    | **Faster.** Treats the context as a single observation (usually by averaging vectors). | **Slower.** Generates more (target, context) pairs per window, requiring more predictions. |\n",
        "| **Representation quality** | Works well for **frequent words.** Averaging may \"smooth out\" contextual information. | Works better for **rare words** and small datasets. Learns better word representations by considering individual (target, context) pairs. |\n",
        "| **Handling rare words**  | Less effective than Skip-gram.               | More effective than CBOW.                  |\n",
        "| **Computational complexity** | Lower.                                  | Higher (since more predictions are required per input word). |\n",
        "| **Example (Window=2)**  | Input: (\"the\", \"quick\", \"fox\", \"jumps\") → Output: \"brown\" | Input: \"brown\" → Output: (\"the\", \"quick\", \"fox\", \"jumps\") |\n",
        "\n",
        "####### **When to Use Which Model?**  \n",
        "\n",
        " **Use CBOW if:**  \n",
        "   - You need **faster training**.  \n",
        "   - Your dataset is **large**.  \n",
        "   - You care more about performance on **frequent words**.  \n",
        "\n",
        "**Use Skip-gram if:**  \n",
        "   - You have a **smaller dataset**.  \n",
        "   - You want **better representations for rare words** or specialized phrases.  \n",
        "   - You prioritize **representation quality** over training speed.  \n",
        "\n",
        "### **Summary:**  \n",
        "CBOW is **faster** and better suited for frequent words by predicting the target word from context. Skip-gram is **slower** but more effective for rare words, producing higher-quality word representations by predicting context words from a target word."
      ],
      "metadata": {
        "id": "Z6yc94DHI1pv"
      }
    }
  ]
}